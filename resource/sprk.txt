git账号 871905286

MR程序运行模式，yarn只管理资源分配，他不管理map，reduce，以及shuffle，甚至不指导怎么启动他们
	先走main方法，根据代码里面job的描述然后形成配置文件，以及任务的规划，然后把这些和JAR包一起提交给集群
	当走到job.waitforcompletion()---->RunJar向RM申请以及Job--->返回Job相关的提交路径 stag-dir和JobID-->提交资源到hdfs/tmp/xxx/xxx./yarn-stagging/jobId-->RunJar向RM汇报提交的结果-->RM把本次job加入到队列中；所有的nodemanager会通过心跳机制去检查RM的job队列，当发现有字的JOB时就领取任务-->NM初始化任务环境产生container,同时到/tmp/xxx/xxx./yarn-stagging/jobId 拉去JOB配置以及Jar--->启动MRAppMaster,|||||||yarn的工作到此完成，接下来的任务交给MRAppMaster
	MRAppMaster--->向RM注册，同时获取执行本次Job的运行节点-->然后MRAppMaster在节点上启动mapTask（进程yarn child）任务，同时MRAppMaster监控mapTask的运行状况(异常和正常结束)，---->此过程见下方shuffle过程-->当maptask结束之后，向APPMaster汇报结果(结果文件的位置，以及分区信息)---->所有的map任务结束之后，APPMaster启动reduceTask-->
	---->通知reduceTask改处理那个以及这个分区的所有数据--->ReduceTask下载数据--->再次合并排序，在处理数据---->结果输出---->任务完成向RM注销自己的，RM回收资源

	RunJar中持有RPC的通讯客户端，与RM提供具体服务的对象实现同一个对象，实现统一接口，通过这个和RM通讯

shuffle
	map
		每个MapTask有一个自己独立的环形缓存区，默认大小100M 有配置文件中io.sort.mb 决定，一旦达到阀值得0.8(这个值可以通过io.sort.spill.percent)来决定，一个后台线程会把内容写入到磁盘的指定目录(mapred.local.dir)。下一个新建一处文件
		文件中就是一个个ker-value ，而且写的时候事先分组，某一段就是同一个组的，每一组会根据KEY拍好序（先分区，后排序），这个文件不会太大，文件达到配置的大小，立马新建一个文件
		map完了之后，会把所有的小文件合并，合并的时候，会再一次排序，分组，最终，一个文件一个组。
	reduce
		每个reduce处理一组数据，他会去所有的map节点上拉去同一个分组的数据，通过Http的方式
		然后再把所有同一个分组的数据，做全局排序

：distinct、 groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等
spark-shell界面:load 加上.scala文件结尾的文件全路径，即可在spark shell中执行scala文件
如果是用yarn-client模式提交，那么本地是直接可以看到log的,可以在本地看到所有的log
如果是用yarn- cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，
我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。
提高shuffle操作的并行度,使用Hive ETL预处理数据,过滤少数导致倾斜的key
两阶段聚合:这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机 数，比如10以内的随机数，
此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，
就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，
进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，
就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。

将reduce join转为map join,广博局部变量

reduce：内部调用了runJob方法，是一个action操作。 
reduceByKey：内部只是调用了combineBykey，是Transformation操作。

大量的数据操作时，reduce汇总所有数据到主节点会有性能瓶颈，将数据转换为Key-Value的形式使用reduceByKey实现逻辑，
会做类似mr程序中的Combiner的操作，Transformation操作分布式进行。

sparkShuffle分为 hashShuffle 和 SortShuffleManager

spark stand alone 配置
	slaves
    
spark 数据倾斜，数据来源于hive表，可以使用hive预处理数据，然后在Spark作业中针对的数据源就不是原来的Hive表了，
而是预处理后的 Hive表。此时由于数据已经预先进行过聚合或join操作了	

比如 reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，
比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，
该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。
	
spark.shuffle.file.buffer
默认值：32k
参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。
调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。

spark.reducer.maxSizeInFlight
默认值：48m
参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。
调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。

spark.shuffle.io.maxRetries
默认值：3
参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。
调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。

spark.shuffle.io.retryWait
默认值：5s
参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。
调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。

spark.shuffle.memoryFraction
默认值：0.2
参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。
调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。

spark.shuffle.manager
默认值：sort
参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。
调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。

spark.shuffle.sort.bypassMergeThreshold
默认值：200
参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。
调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。

spark.shuffle.consolidateFiles
默认值：false
参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。
调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。
	
job运行流程：
1-------->构建spark Application的运行环境；
                                          在Driver Program中新建SparkContext(包含sparkcontext的程序成为Driver Program);
                                          Spark Application运行的表现方式，在集群上运行着一组独立的executor进程，
										  这些进程Sparkcontext来协调;
2-------->SparkContext向资源管理器申请运行Executor资源，并启动 executor,executor向SparkContext申请task;(exrcutor用来计算或者存储数据)
          Application（拥有属于自己的Executor）
3-------->sparkcontext获取到executor之后，Application的应用代码将会被发送到各个Executor；
4-------->SparkContext构建RDD DAG图分解成stageDAG图，将stage提交给TASKcheduler,最后由TASKcheduler,将task发给executor；
5-------->task在executor上运行，运行完毕后释放所有资源；
partition & Task 一个分区就执行一个任务
	Task
		Task是Executor中执行单元
		Task处理数据常见的两个来源：外部存储以及shuffle数据
		Task可以运行在集群中的任意一个节点上
		为了容错，会将shuffle输出写到磁盘或者内存中

RDD特性（前三个针对任何RDD）：一个分区的集合 RDD的具体值不可以改变，只能转换属性
                              每一个分片上都应用一个函数进行计算（高阶函数）
                              没一个EDD都依赖前面的RDD  
                       针对key--value的RDD可以指定分区函数
                       数据计算本地化      
RDD的操作
spark创建RDD的两种方法 extend class RDD 
SparkContext.textFile("")。。。。。
Parallelized
				val data = Array(1, 2, 3, 4, 5)
				val rdd = sc.parallelize(data)

External 
				val distFile = sc.textFile("data.txt")

RDD依赖 宽依赖和窄依赖
     窄依赖：
			一个RDD有很多个分片  转化之后他们的分区不变
			（join）即一个子RDD可能有多个父RDD，但是一个父RDD只有一子RDD,而宽依赖相反，一个父rdd会有多个子RDD
			 textFile--->wordRDD--->kvRDD窄依赖     kvrdd--->keyRDD宽依赖
===========================================
textRDDR		wordRDD					kvRDD						keyRDD (groupBykey（可以指定分区个数）把相同key弄到一个分区)
--------------------------------------------------------|-------------------------
				spark 						spark,1		|			（spark，list(1,1,1)
	block1		hadoop			map			hadoop,1	|groupBykey	 (hadoop,list(1) 
				hdfs						hdfs,1		|		     
--------------------------------------------------------|---------------------------
				spark						spark,1		|			
	block2		yarn			map			yarn,1		|groupBykey		（hdfs，list（1）	
				rdd							rdd,1		|				（rdd,list(1.1)）
--------------------------------------------------------|
				spark						spark,1		|		
	block3		rdd				map			rdd,1						
==================================================
窄依赖：每个RDD的每个partitiong都依赖父partition的某个partition,父和子RDD的partition一对一关系
	子RDD的每个分区依赖于常数个父分区（即与数据规模无关）
	输入输出一对一的算子，且结果RDD的分区结构不变，主要是map,flatMap
	输入输出一对一，但结果rdd的分区结构发生了变化，如union，coalesce
	从输入中选择部分元素的算子，如filter，distinct，subtract,sample
宽依赖
	没个子RDD的每个分区依赖于所有父Rdd分区
	对单个RDD基于key进行重组和reduce，如groupBykey,reduceByKey;
	对两个RDD基于key进行join和重组，如join

Spark Shuffle	对数据重新分区调整的机制
会引起分区的操作 具有重新分区操作repartition,coalesce;*Bykey:groupByKry,reduceByKey;关联操作：join,cogroup
	默认分区，textFile("文件路径",分区数）默认的分区数 sparkContext def defaultMinpartitions:Int = math.main(defaultParallelism,2)
	默认的并行度和2的最小值
    查看分区数 保存为文件，查看文件个数  saveAsTextFile(As)
repartition--->s  aveAsTextFile在新版本里这个过程在UI分两条显示
=============================================================================

内核
application:用户程序，构建在sprak上的应用程序，每一个包含一个diverprogram 和很多个executors
diverprogram 和 executors属于每个应用程序，且相互独立
diverprogram ：一个进程，运行main方法的进程，并且创建sparkcontext
executors：真正运行任务的进程，属于worker，task:work的单元
clustermanager:相当于nodemanager，管理分配资源 
//diver在集群上的就是集群模式，diver

spark架构原理，组件：Dirver、Master、Worker、Executor、Task
	当Dirver启动后会做一些初始化操作，这时候会发送请求到master上，进行spark程序的注册，其实告诉master有程序要运行，
	之后master分配资源，之后启动exector当exector启动后会在向diver反注册一些东西。当diver上注册了一些Exector
	之后就可以正式开始我们的spark程序了，第一步创建初始RDD读取数据源，数据直接被读取到多个Worker节点上形成分布数据集RDD,
	Diver会根据task到exector,exector接收到task后会启动多个线程来执行task
	之后task就根据core里面的transfer算子进行RDD进行转换，或去其他的RDD 

	spark-submit，构造出一个dirveractor进程（用反射），之后sparkcontext被构造
	初始化sparkcontext的时候构造DAGScheduler和taskScheduler,taskScheduler负责通过他的后台进程链接master，向master注册application，
	之后master会去联系worker，之后worker会启动exector，executor启动后去taskScheduler反向注反向注册
	所有的executor都在dirver上反向注册之后，sparkcontext初始化完成。之后执行自己开发的代码
	每执行一个到一个sparkcontext,就创建一个Job，job会提交个DAGscheduler,DAGScheduler会将每个job划分为多个stage，
	然后每个stage会创建一个taskset，之后交给TaskScheduler,之后taskscheduler,会把taskset里面的每个task提交到exector上执行
	exector每接受到一个task,都会用taskrunner封装task,然后从线程池里取出一个线程，执行这个task,taskruner将我们编写的代码，
	也就是要执行的算子以及函数拷贝，反序列化，然后执行task
	
	task有两种shuffleMapTask和ResulterTask,只有最后一个stage是resultTask，之前的是另一个，所以整个spark应用程序的执行，
	就是stage分批次作为taskset提交到executor执行，每个task针对RDD的一个partition，执行我们的算子和函数，以此类推知道所有操作完成
		
	Dirver集群节点之一/一般是提交Spark程序的机器，自己编写的程序mian方法由Dirver执行。
		
	Master,资源的调度分配，还有集群的监控
		Master支持热备，sparkMaster切换模式一种与文件的，需要手动，一种是zookeeper自动切换
		使用持久化引擎去读取持久化的storeApps,storeDrivers,storeWorkers,FileSystemPersistenceEngine,zookeeperPersistenceEngine,-->
		判断storeApps,storeDrivers,storeWorkers,有任何非空--->将持久化的Application,Dirver,worker 的信息，重新注册，
		注册到master的内存中。
 	
	Worker 存储RDD的某个partion,启动线程对RDD的partion进行计算	
	启动Exector进程，exector再启动task,然后负责对RDD的partion进行计算
	
sparkContext创建三个东西:
	taskScheduler(初始化):构建taskScheluderImpl对象（相当于taskScheluder），依赖于（SparkDeploySchedulerBackend）
					创建SparkDeploySchedulerBackend(他在底层负责taskScheluderImpl的控制，实际上是负责与Master的注册，
					executor的反注册，task发送到executor等操作，)
					之后调用taskScheluderImpl.init()方法，会创建SchedulerPool
			taskScheduler创建完成后，会调用taskScheluderImpl.start方法--->SparkDeploySchedulerBackend.start()-->AppClient--->
			创建一个ClientActor--->registerWithMaster()--->tryRegisterWithMaster()---RegisterApplication
			(case class 里面封装了application的信息-->spark集群，Master--Worker--exector
	
	DAGScheduler:创建DAGSchedulerEvevtProcessActor DAGScheduler的底层基于该组件通讯
	
	sparkUI 4040端口

Spark Scheduler(Spark 调度)
		建任务————>RDD Object——————(DAG)————>DAGScheduler————(Taskset)————>TaskScheduler——————(Task)————>Worker  (带括号的是过度单位)
		RDD Object 实质是构建RDDDAG图，以及依赖关系 build operator DAG
		 
		DAGScheduler 划分阶段（stage）按照是否有shuffle划分，如果有则就分开,引起shuffle的操作 Split graph into stages of tasks,submit each stage as ready.
			会引起shuffle的操作
			    从新分区reparition,coalesce
				*BeyKey groupByKey,reduceByKey
				关联操作 join,cogroup
			阶段任务		
				接收用户提交得到job
				构建stage，记录那个RDD或者Stage输出被物化
				重新提交shuffle输出丢失的stage
				将Taskset传给底层点调度器
			
		Task Scheduler调度任务申请资源 ，向driverProgram中的sparkContext申请资源，申请到资源后把taskSet中的文物放在调度器中
		（Schedulable Buildes） 然后发送到各个work节点 Executor Task
			launch tasks via cluster manager,retry failed or straggling tasks
				提交taskset(一组 task)到集群运行并监控
				为每一个TaskSet构建一个TaskSetManager实例，管理这个TaskSet的生命周期
				数据本地性决定每个Task最贱位置（process-local,node-local,rack-local and then any）
				推测执行，碰到straggle任务需要放到别的节点上重试，出现shuffle输出lost要报告fetch failed错误

		Worker execute tasks ,store and serve blocks  (包含Threads Block manager)
	
启动spark-shell 会读取spark-defaults.conf   

二次排序   
分组 val text = sc.textFile("hdfs://192.168.126.3:8020/text").map(_.split(" ")).map(x=>(x(0),x(1)))..groupByKey
		RDD[Array[String]] RDD[(String, String)] (aa,CompactBuffer(34, 45, 45))
Iterable 方法 toList()  toArray


提交 (stand alone)
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]

./bin/spark-submit \
  --master spark://192.168.126.3:7077 \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]

--deploy-mode选项 制定the driver program(运行在哪里)  
	client 本地，运行的着他机器
	cluster（在某一台worker上）
	
spark on yarn   （spark-shell --master yarn-master  (用于，生产环境，缺点在于调试不方便，本地本能看到log,log会在rm中)
	yarn 管理整个集群管理资源  nodemanager
	任务调度：ApplicationMaster,Container
		1)client -> RM--->分配container      RM相当于master  AM相当于dirver,   NM相当于worker 
		2)RM -> APPMaster（相当于dirver）-->RM请求contianer--->启动executor（NM）
		3) Spark APP -> Executor
			SparkAppMaster -> RM 申请资源
		4）Driver Progrm(管理整个用用)
			 DAG -> DAGScheduler > StageScheduler ＞　TaskSet
		
	--master yarn-client  ,只有在申请资源的时候才会联系yarn,然后用container去联系nodemanager,（用于测试，会和yarn产生大量的网络通讯，
	但是会在本地看到所有的log,）
	bin/spark-submit  \
	
	--master yarn-cluster \
	<jar 包>

如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn- cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage
	
算子 map() flatMap()......都为算子

rdd持久化。
	大量数据操作的RDD图中的不同位置，有相同的操作，如果没有rdd持久化，相同的操作每走一次就会从最开时的文件开始计算，
	会非常耗时所以调用cache()或persist(StorageLevel)把那些多次用到的rdd持久化
	StorageLevel(枚举)用于选择持久化的地方，内存还是磁盘

共享变量某个算子使用到外部的变量
	Broadcast Variable 广播变量，只读，会被拷贝一份到所有的节点上去到所有的节点上去
		共享变量val a = sc.broadcast(变量)
		使用a.value()
		
	Accumulator共享变量，可写，只提供了累加功能，却可以多个task对一个变量并行操作，但是只有Dirver才可以读取
			sc.accumulator(变量)

spark 性能调优
	内存开销	
			1、每个java对象在内存中存储时都会有mark word(对象头)，占用额外的内存。
			2、java的String对象。会比他内部的原始数要多出40个字节，因为他的内部是char数组 用UTF-16编码，每个个字符会展2个字节
			3、java的结合类型，比如HashMap和LinkedList,内部使用的是连班数据结构，所以对链表中的每个数，都使用Entry对象来包装。
				Entry对象不光有对象头，还有指针，通常占用8个字节
			4、元素类型为原始类型的几何，内部同样会使用包装类。

	判断应用程序消耗多内存，			
			首先设置RDD的并行度，他也是partition的数量。			
			然后将RDD cache到内存中，调用。cache()
			然后观察Dirver日志
	
	使用高性能序列化机制 Kryo序列化机制
		使用场景：算子函数使用到外面的大数据情况，比如说外面自定义了一个MyConfiguration对象，里面包装了100M的数据，算子里面使用到外部大对象。
			1.new SparkConf().set("soark.serializer","org.apache.spark.Serializer.KryoSerializer")	
			2.对需要序列化的类进行注册注册
			    val conf = new SparkConf（）....
				conf.registerKryoClasses(Array(classOf[Counter])) Counter  类名
				SparkContext = new 。。。。。。
					
					当要序列化的类过大，优化kryo类库，默认是2M 调用SparkConf().set("spark.kryoserializer.buffer.mb",10)
				
	优化数据结构有先使用数组以及字符串，而不是用和	企业级的HashMap,list这种数据用Sring 数据,避免使用嵌套对象,尽量使用jsion，尽量使用int 代替String
	
	对多次使用的RDD进行持久化或Checkpoint  持久化化后RDD会放入到BlockManager中，但是有可能会导致blickManager中的数据丢失了，
	此时会尝试读取Checkpoint，如果也没有，则计算
		那么在第一次计算完成后就执行checkPiont到HDFS上去
		如果内存不够就使用序列化(memory_only)
	
	gc调优 GC是一个线程，gc运行的时候，会让工作线程停下来，频繁发生gc会导致工作线程频繁停下来
		优化exceutor缓存比例
			默认情况下executor的内存空间划了60%地	RDD进行缓存，比如说我们执行RDDcache的时候，那么这个exrcutor上的RDDpartition,
			则cache会占用60%的存储空间，40%的存储空间给了task
			task在运行的时候回创建很多对象，默认情况下task的内存偏小,内存容易满，所以gc会频繁发生，就会导致task工作效率
			new SparkConf().set("spark.storage.memoryFaction", "o.5")(rdd数据的缓存比例)，分配给task的内存空间。其实是jvm的堆空间大小
			jvm的堆空间分成了两块。一个是年轻代，一个是老年代，年轻带是短时间存活的对象，老年代是长时间存货的对象，
			年轻带又被分为三块，EDEN,survivor1,survivor1,eden区域最大，首先，创建的对象都是放在eden和survivor1中，
			survivor2是作为备用的。当eden区域满了之后机会发生minor gc 操作，会回收新生代中不在使用的对象。
			此时eden和survivor1中存活的对象会移入survivor2中老年代  如果一个新生代中一个对象，多次minor都没有将其清楚，
			说明他是长时间存货的对象，那么就移入老年代，特殊情况，在minor gc 发生的时候将存活的对象放入备用survivor区域，
			结果发现survivor满了，就会放入老年代中，所以就会出现短时间存活的对象进入老年代，浪费内存空间，而且老年代会占满，
			就会出现full gc ,回收老年对象，导致task频繁发生，full比较慢
			使用-Xmn即可，将预计设置为预计大小的4/3  在spark-submit脚本中加入 
			--conf"spark.executor.extra.JavaOptions=-verbose:gc-Xmn:+大小"，其实根据经验来看，调整内存的比例就可以了，
			除非是万不得已，
	
	提高并行读
		new SparkConf().set("spark.default.parallelison","5"),这个参数一旦设置，那么每个RDD的数据会被分为5分，则就会有5个task来计算RDD，
		最好不要让CPU的core空闲，官方建议设置为计算机cpu核数的2到3倍
		
	广播共享数据，
		val myconf = new MyConfiguration()
		val rdd = ...map(sle=>myconf。。。)
		算子使用到外部的数据，避免每个task拷贝数发生网络开销，这时候可以使用broakast广播，然后让他在每个副本上都有一个副本
	
	数本地化
		PROCESS_LOCAL:数据和代码在同一个jvm中。
		NODE_LOCAL:数据和计算他的代码在同一个节点上，但是不在同一个进程中，比如在不同的executor进程，或者是在hdfs的bolok中。
		NO_PREF,数据在哪里，task就在哪里,不区分。
		RACK_LOCAL：数据和计算的代码在同一个机架上。
		
	能用reduceByKey就用	reduceByKey，只有在reduceByKey处理不好的时候在用groupByKey
	
shuffle优化
	new SparkConf().set("spark.shuffle.consolidateFiles","true") 开启consolidation机制
		每一个mapTask，都会为所有的reduce都创造一个文件，没有开启consolid机制的时候shuffle 读写的性能，因为在shuffleMap端创造的文
		件太多了导致shuffle的写，对于shuffle读也是一样的，
		开启consolidation机制后没个shffule Map 写文件的个数大大减少，只有在并行的文件才创建文件，不并行的task会服用而文件 
	
	spark.reducer.maxSizeinFight
		加大reduce端的缓存大小，默认48M，用来缓存从map端的数据
	
	spark.shffule.file.buffer
		加大map端的缓存大小，用来缓存map处理完要写入磁盘的数据 默认32K
	
	spark.shuffle.me.moryFraction
		在执行reduceTask的时候，有一部分内存用来汇聚各个map端的数据，默认是0.2,超过的时候回写入磁盘中， 
	
	spark.shuffle.io.maxRetris
		拉去失败的最大次数，默认3次
		
	spark.shuffle.io.retryWait
		拉去失败时重是间隔。
		
二次排序  
    自定义key 
	class SecondSortKey(val first: Int,val second:Int ) extend Ordered[SecondSortKey] with Serializable{
		def compare (that : SecondSortKey)：Int = {
			if(this.first - that.first !=0){
				this.first - that.first
			}else{
				this.second  - that.second
			}
		}
	}
	//其中的compare方法会在sortByKey时调用，所以只要把每一行的数据映射成一个自定义的key对象即可

	
[(_,_)]groepByKey()空参    [(_,_)]reduceByKey(fun)  [(Int,String)]sortByKey(boolean)
join是根据keyjoin的 rdd1[(1,2)]   rdd2[(1,3)]    rdd1.join(rdd2)返回[1,(2,3)]  
cogroup 和join不同的是会把所有的key相同的value放入到一个集合中。
分组topN
	top N 用到的方法 Iterable[].tolist  take(n)	
	
Streaming 建立在Spark-core 之上  Streaming Context 底层封装了Spark Context
     Data Stream are chopped up into Batches (数据分片), each batch processed in Spark core(RDD) 返会
	  
Streaming  app 会在 Executor 上运行 一个Receiver  然后会把每一个block(bacth) 变成一个（RDD） 交给ssc（Streaming Context） 然后 ssc  跑 Spark Context
	
	Streaming socket （开发） 
			import org.apache.spark._
			import org.apache.spark.streaming._
			
			val conf = new SparkConf().setAppName()....
			val ssc = new StreamingContext(conf,Seconds(1))
			
			// read data
			val lines = ssc.socketTextStream("IP",port)
			//process
			val wordRDD = lines.flatMap(_.split（“”）)
			。。。。。。。。。
			
			//固定格式
			ssc.start()
			ssc.awaitTermination()
					
	Streaming hdfs （开发） 
			import org.apache.spark._
			import org.apache.spark.streaming._
			
			val conf = new SparkConf().setAppName()....
			val ssc = new StreamingContext(conf,Seconds(1))
			
			// read data（会忽略一点开头的文件）
			val lines = ssc.textFRileStream("hdfs://dsg:8020/file/")
			//process
			val wordRDD = lines.flatMap(_.split（“”）)
			。。。。。。。。。
			
			//固定格式
			ssc.start()
			ssc.awaitTermination()

	spark fulme push、
	     flume 将数据写到一个端口 
		 sink配置
			a2.sinks.k2.type = avro
			a2.sinks.k2.hostname = hostname
			a2.sinks.k2.port = 00000
			
		代码
			val conf = new SparkConf().setAppName()....
			val sc = new SparkContext(conf)
			val ssc = new StreamingContext(sc,Seconds(1))
			//read data
			val stram = FlumeUtils.createStream(ssc,hostname,port,StorageLevel)
			stream.count.map(........)
			
			ssc.start()
			ssc.awaitTermination()
		
sparkSQL
	特点
		1、支持多种数据源，Hive,RDD,parquet,json,jdbc等
		2、多种性能优化技术：in-memory columnar story(内存列存储)、byte-code generation(字节码生成技术)、cost mode动态评估。
		3、组件扩展性：对于SQL的语法解析器、分析器、优化器，用户都可以自己在重新开发，并且动态扩展。
	
	DataFrame,可以理解为是，以列的形式组织的，分布式数据集合。和关系型数据库中的表非常类似，但是底层做了很多优化。
	DataFrame可以通过很多源来进行构建，包括：结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD
		val sc: SparkContext = ......
		val sqlContext = new SQLContext(sc)
		import sqlContext.inplivits._
		DataFrame df = sqlContext.read.json("hdfs://....")
		df.show()//打印所有的数据
		df.printSchema()//打印元数据
		df.select("name").show//查询某列的所有数据
		df.select(df.col("name"),df.col("age").+1).show()//查询某几列的所有数据数据，并对列进行计算
		df.filter(df.col("age")>19).show//根据某一列的数据进行过滤
		df.groupBy("age").count.show//根据某一列进行分组，然后聚合
	
	
  /opt/modules/spark-1.6.1-bin-2.5.0-cdh5.3.6/bin/spark-submit \
  --master yarn-master \
  --class com.sql.JsonSql \
  --num-executors 3 \
  --driver-memory 540m \
  --executor-memory 540 \
  --executor-cores 3 \
  /home/ligeng/jar/untitled.jar
	
	RDD TO DataFrame  （把数据映射为一个对象）(Scala 不能用def main()的方法来运行程序，否则报 no type for .... class ,要用 Object extend App)  
		import sqlContext.implicits._
		val sc: SparkContext = ......
		val sqlContext = new SQLContext(sc)
		
		case class Student(id:Int,name :String,age :Int)
		
		val students = sc.textFile(" ......   ").map(line => line.split(" ")).map(arr=>Student(arr(0),....,))/把数组变成对象
		//直接用RDD的同DF，即可把RDD转化为DataFrame 
		val df = students.toDF
		//然后注册临时表
		df.registerTempTable("students")
		
		val sql = sqlContext.sql("select * from students where age <=18")
		//又转换为Rdd
		sql.rdd.map
						demo
										import org.apache.spark.{SparkConf, SparkContext}
										import org.apache.spark.sql.SQLContext
										object BeanSql extends App{
										  val con = new SparkConf().setAppName("BeanDFSQL").set("spark.testing.memory", "540000000").setMaster("local")
										  val sc = new SparkContext(con)
										  val sqlContext = new SQLContext(sc)
										  import sqlContext.implicits._
										  case class Student(id:Int,name :String,age :Int)
										  val students = sc.textFile("D://test//dongtaichuangjianSQL.txt",1).map(_.split(" ")).map(arr=>Student(arr(0).toInt,arr(1),arr(2).toInt))
										  val df = students.toDF
										  df.registerTempTable("students")
										  val sql = sqlContext.sql("select * from students where age<=20")
										  sql.rdd.collect.foreach(x=>print(x(1)))
										}
		
	RDD TO DataFrame ，动态构建元数据表（无对象）
		import sqlContext.implicits._
		val sc: SparkContext = ......
		val sqlContext = new SQLContext(sc)
		
		//第一步构建出元素为Row的普通RDD
		val students = sc.textFile(" ......   ").map(line =>Row(line.split(" ")(0).toInt),.....,))
		
		//构建元数据
		val structType = StructType(Array(
			StructField("id",IntegerType,true),
			StructField("name",StringType,true),
			StructField("age",IntegerType,true)
		))
		
		//把RDD转化为DataFrame
		val df = sqlContext.createDataFrame(students,structType)
		
		//然后注册临时表
		df.registerTempTable("students")
		
		val sql = sqlContext.sql("select * from students where age <=18")
		//又转换为Rdd
		sql.rdd.map
						demo
									import org.apache.spark.sql.types.{StructType,StructField,IntegerType,StringType}
									import org.apache.spark.sql.{Row, SQLContext}
									import org.apache.spark.{SparkConf, SparkContext
									object DongtaiSQL extends App{
									  val con = new SparkConf().setAppName("DongtaiSQL").set("spark.testing.memory", "540000000").setMaster("local")
									  val sc = new SparkContext(con)
									  val sqlContext = new SQLContext(sc)
									  val students = sc.textFile("D://test//dongtaichuangjianSQL.txt",1).map(_.split(" ")).map(arr=>Row(arr(0).toInt,arr(1),arr(2).toInt))
									  val structType = StructType(Array (StructField("id",IntegerType,true),StructField("name",StringType,true), StructField("age",IntegerType,true)))
									  val df = sqlContext.createDataFrame(students,structType)
									  df.registerTempTable("students")
									  val sql = sqlContext.sql("select * from students where age <=18")
									  sql.rdd.collect.foreach(x=>print(x(1)))
									}

DataFrame load save  (parquet文件离线操作会有权限麻烦)
	load
		val sqlContext = 。。。。。。。。。
		val df = sqlContext.read.json("hdfs://DSG:8020/test/input/jsonsql.json")//load json 文件
		
		val df = sqlContext.read("。。。。.parquet")//load parquet文件
		df.write.save("路径")
		
		手动指定数据源类型 （可以就行不同数据源之间转化）
		val df = sqlContext.read.format("json").load(".........")
		df.write.format("parquet").save(".................")
		
	save model  （用来处理当目标已经有数据的时候的处理方案）	
		df.save("路径"，SaveModel.ErrorIfExits)(默认)
		SaveModel.Append 追加
		SaveModel.OverWrite 覆盖
		SaveModel.Ignore 忽略不做任何操作
		
Parquet数据源
		val sqlContext = 。。。。。。。。。
		val df = sqlContext.read.parquet("hdfs://DSG:8020/test)//读取parquet
		df.registTempTable("user")//注册临时表
		val sql = sqlContext.sql("select * from user")
		sql.rdd.map{x=>print("_");x}
		sc.stop
	parquet数据源自动分区推断
		hdfs目录结构:  /test/user/gender=male/country=us
		数据放在上面的目录下
		则数据在读取之后会自动加入 gender 和 country 两列
	
	SparkSQL 可以自动推断JSON的元数据，并加载数据SQLContext.read.json(),danshi zheli de json数据每一行只能包含一个单独的对象，不能把一个队形分在多行
		案例：查询分数大于80分的学生详细信息
	
Hive 数据源
	//1、创建SparkConf
	//创建HiveContext
	val hc = new HiveContext(New SparkContext(conf))
	
	DataFram df = hc.sql("hql") 
	df.saveAsTable("table name ")//创建一个永久存在的表
	
	//可以用hc.table()方法针对Hive表里面的数据直接创建DF
	
JDBC 数据源	
	val conf = new?SparkConf().setAppName("mysql").setMaster("local")
	val sc = new SparkContext(conf)
	val sqlContext = new org.apache.spark.sql.SQLContext(sc)
	
	val jdbcDF = sqlContext.read.format("jdbc").options(
	Map("url"->"jdbc:mysql://localhost:3306/db_ldjs",
	"dbtable"->"(select imei,region,city,company,name?from tb_user_imei) as some_alias", 
	"driver"->"com.mysql.jdbc.Driver",
	"user"-> "root",
	"password"->"123456")).load() 
	
